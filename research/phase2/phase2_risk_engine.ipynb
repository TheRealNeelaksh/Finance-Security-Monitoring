{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d6fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\anaconda3\\envs\\abhishek_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Running in interactive mode. Assuming current directory is the script location.\n",
      "üìÇ Script Location: c:\\Users\\abhis\\OneDrive\\Desktop\\Models\\phase2\n",
      "üìÇ Loading Data from: c:\\Users\\abhis\\OneDrive\\Desktop\\Models\\phase1\n",
      "\n",
      "1. Consolidating outputs from all models...\n",
      "   ‚úÖ Loaded Network Graph scores.\n",
      "2. Generating Meta-Features (Model Scores)...\n",
      "   ‚úÖ Loaded Phase 1 behavior models.\n",
      "   Re-calculating features for inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:04<00:00, 4313.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Running Isolation Forest Inference...\n",
      "   Running Autoencoder Inference...\n",
      "   Simulating LSTM Session scores...\n",
      "\n",
      "3. Training the Master Ensemble (XGBoost)...\n",
      "\n",
      "4. Ensemble Model Evaluation:\n",
      "   Accuracy: 86.75%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92      3185\n",
      "           1       0.92      0.38      0.54       815\n",
      "\n",
      "    accuracy                           0.87      4000\n",
      "   macro avg       0.89      0.69      0.73      4000\n",
      "weighted avg       0.87      0.87      0.84      4000\n",
      "\n",
      "\n",
      "Feature Importance:\n",
      "   score_if: 0.1704\n",
      "   score_ae: 0.0262\n",
      "   score_lstm: 0.7859\n",
      "   network_risk_score: 0.0174\n",
      "\n",
      "‚úÖ Risk Engine Saved at: c:\\Users\\abhis\\OneDrive\\Desktop\\Models\\phase2\\model_risk_engine.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\anaconda3\\envs\\abhishek_gpu\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:14:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import os  # <--- Added OS module\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from geopy.distance import geodesic\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# ==========================================\n",
    "# 0. SETUP DYNAMIC PATHS\n",
    "# ==========================================\n",
    "try:\n",
    "    # This gets the directory where THIS script is located (e.g., .../Project/phase2)\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # Fallback for Jupyter Notebooks where __file__ is not defined\n",
    "    # We assume the notebook is running from the 'phase2' directory\n",
    "    BASE_DIR = os.getcwd()\n",
    "    print(\"‚ö†Ô∏è Running in interactive mode. Assuming current directory is the script location.\")\n",
    "\n",
    "# We need to go UP one level to find 'phase1' (e.g., .../Project/phase1)\n",
    "PROJECT_ROOT = os.path.dirname(BASE_DIR)\n",
    "PHASE1_DIR = os.path.join(PROJECT_ROOT, 'phase1')\n",
    "\n",
    "print(f\"üìÇ Script Location: {BASE_DIR}\")\n",
    "print(f\"üìÇ Loading Data from: {PHASE1_DIR}\")\n",
    "\n",
    "# ==========================================\n",
    "# PART 1: LOAD & CONSOLIDATE DATA\n",
    "# ==========================================\n",
    "print(\"\\n1. Consolidating outputs from all models...\")\n",
    "\n",
    "# A. Load Base Data (User Logins)\n",
    "# We look for the file inside the phase1 folder\n",
    "logins_path = os.path.join(PHASE1_DIR, 'user_logins.csv')\n",
    "\n",
    "if not os.path.exists(logins_path):\n",
    "    print(f\"‚ùå Error: Could not find {logins_path}\")\n",
    "    print(\"   Make sure 'user_logins.csv' is inside the 'phase1' folder.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(logins_path)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.sort_values(by=['user_id', 'timestamp'])\n",
    "\n",
    "# --- FIX: Handle Column Names ---\n",
    "if 'device_user_agent' not in df.columns and 'device' in df.columns:\n",
    "    df.rename(columns={'device': 'device_user_agent'}, inplace=True)\n",
    "\n",
    "# B. Load Network Scores (Phase 1.C)\n",
    "network_path = os.path.join(PHASE1_DIR, 'network_risk_scores.csv')\n",
    "\n",
    "if os.path.exists(network_path):\n",
    "    network_scores = pd.read_csv(network_path)\n",
    "    df = pd.merge(df, network_scores, on='user_id', how='left')\n",
    "    df['network_risk_score'] = df['network_risk_score'].fillna(0)\n",
    "    print(\"   ‚úÖ Loaded Network Graph scores.\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Network scores not found. Defaulting to 0.\")\n",
    "    df['network_risk_score'] = 0.0\n",
    "\n",
    "# ==========================================\n",
    "# PART 2: GENERATE SCORES FROM PHASE 1 MODELS\n",
    "# ==========================================\n",
    "print(\"2. Generating Meta-Features (Model Scores)...\")\n",
    "\n",
    "# Define Model Paths\n",
    "iso_path = os.path.join(PHASE1_DIR, 'model_isolation_forest.pkl')\n",
    "scaler_path = os.path.join(PHASE1_DIR, 'scaler.pkl')\n",
    "ae_path = os.path.join(PHASE1_DIR, 'model_autoencoder.h5')\n",
    "\n",
    "# A. Load Models\n",
    "try:\n",
    "    iso_forest = joblib.load(iso_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    autoencoder = load_model(ae_path)\n",
    "    print(\"   ‚úÖ Loaded Phase 1 behavior models.\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå CRITICAL: Could not load models from {PHASE1_DIR}\")\n",
    "    print(f\"   Error Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# B. Re-Engineer Features (Same logic as before)\n",
    "print(\"   Re-calculating features for inference...\")\n",
    "\n",
    "# 1. Time Diff\n",
    "df['prev_time'] = df.groupby('user_id')['timestamp'].shift(1)\n",
    "df['time_diff_hours'] = (df['timestamp'] - df['prev_time']).dt.total_seconds() / 3600\n",
    "df['time_diff_hours'] = df['time_diff_hours'].fillna(0)\n",
    "\n",
    "# 2. Velocity\n",
    "df['prev_lat'] = df.groupby('user_id')['lat'].shift(1)\n",
    "df['prev_lon'] = df.groupby('user_id')['lon'].shift(1)\n",
    "\n",
    "def get_geo_dist(row):\n",
    "    if pd.isna(row['prev_lat']): return 0.0\n",
    "    try:\n",
    "        return geodesic((row['prev_lat'], row['prev_lon']), (row['lat'], row['lon'])).km\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "df['dist_km'] = df.progress_apply(get_geo_dist, axis=1)\n",
    "df['velocity_kmh'] = df['dist_km'] / (df['time_diff_hours'] + 0.1)\n",
    "\n",
    "# 3. Device Trust\n",
    "device_counts = df.groupby(['user_id', 'device_user_agent']).size().reset_index(name='count')\n",
    "total_counts = df.groupby('user_id').size().reset_index(name='total')\n",
    "device_stats = pd.merge(device_counts, total_counts, on='user_id')\n",
    "device_stats['device_trust_score'] = device_stats['count'] / device_stats['total']\n",
    "df = pd.merge(df, device_stats[['user_id', 'device_user_agent', 'device_trust_score']], \n",
    "              on=['user_id', 'device_user_agent'], how='left')\n",
    "\n",
    "# 4. Hour\n",
    "df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Prepare Features\n",
    "features_p1 = ['velocity_kmh', 'time_diff_hours', 'device_trust_score', 'hour_of_day']\n",
    "X_behavior = scaler.transform(df[features_p1])\n",
    "\n",
    "# --- SCORE 1: Isolation Forest ---\n",
    "print(\"   Running Isolation Forest Inference...\")\n",
    "iso_preds = iso_forest.predict(X_behavior)\n",
    "df['score_if'] = np.where(iso_preds == -1, 1.0, 0.0)\n",
    "\n",
    "# --- SCORE 2: Autoencoder ---\n",
    "print(\"   Running Autoencoder Inference...\")\n",
    "reconstructions = autoencoder.predict(X_behavior, verbose=0)\n",
    "mse = np.mean(np.power(X_behavior - reconstructions, 2), axis=1)\n",
    "df['score_ae'] = mse\n",
    "\n",
    "# --- SCORE 3: LSTM (Sequence Simulation) ---\n",
    "print(\"   Simulating LSTM Session scores...\")\n",
    "df['score_lstm'] = 0.05 \n",
    "high_risk_sequences = ['Brute Force Success', 'Device Spoofing']\n",
    "df.loc[df['attack_type'].isin(high_risk_sequences), 'score_lstm'] = 0.95\n",
    "\n",
    "# ==========================================\n",
    "# PART 3: TRAIN THE RISK ENGINE (XGBoost)\n",
    "# ==========================================\n",
    "print(\"\\n3. Training the Master Ensemble (XGBoost)...\")\n",
    "\n",
    "ensemble_features = ['score_if', 'score_ae', 'score_lstm', 'network_risk_score']\n",
    "X = df[ensemble_features]\n",
    "y = df['is_attack']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================\n",
    "# PART 4: EVALUATION & SAVE\n",
    "# ==========================================\n",
    "print(\"\\n4. Ensemble Model Evaluation:\")\n",
    "preds = xgb_model.predict(X_test)\n",
    "print(f\"   Accuracy: {accuracy_score(y_test, preds)*100:.2f}%\")\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# Feature Importance\n",
    "importance = xgb_model.feature_importances_\n",
    "print(\"\\nFeature Importance:\")\n",
    "for i, feat in enumerate(ensemble_features):\n",
    "    print(f\"   {feat}: {importance[i]:.4f}\")\n",
    "\n",
    "# Save Model in the CURRENT directory (phase2)\n",
    "save_path = os.path.join(BASE_DIR, \"model_risk_engine.json\")\n",
    "xgb_model.save_model(save_path)\n",
    "print(f\"\\n‚úÖ Risk Engine Saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee9c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abhishek_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
